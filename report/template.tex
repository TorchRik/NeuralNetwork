% Это основная команда, с которой начинается любой \LaTeX-файл. Она отвечает за тип документа, с которым связаны основные правил оформления текста.
\documentclass{article}

% Здесь идет преамбула документа, тут пишутся команды, которые настраивают LaTeX окружение, подключаете внешние пакеты, определяете свои команды и окружения. В данном случае я это делаю в отдельных файлах, а тут подключаю эти файлы.

% Здесь я подключаю разные стилевые пакеты. Например возможности набирать особые символы или возможность компилировать русский текст. Подробное описание внутри.
\usepackage{packages}

% Здесь я определяю разные окружения, например, теоремы, определения, замечания и так далее. У этих окружений разные стили оформления, кроме того, эти окружения могут быть нумерованными или нет. Все подробно объяснено внутри.
\usepackage{environments}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
% Здесь я определяю разные команды, которых нет в LaTeX, но мне нужны, например, команда \tr для обозначения следа матрицы. Или я переопределяю LaTeX команды, которые работают не так, как мне хотелось бы. Типичный пример мнимая и вещественная часть комплексного числа \Im, \Re. В оригинале они выглядят не так, как мы привыкли. Кроме того, \Im еще используется и для обозначения образа линейного отображения. Подробнее описано внутри.
\usepackage{commands}

% Потребуется для вставки картинки подписи
% \usepackage{graphicx}

% Пакет для титульника проекта
\usepackage{titlepage}

% Здесь задаем параметры титульной страницы
\setUDK{192.168.1.1}
% Выбрать одно из двух
% \setToResearch
\setToProgram

\setTitle{Нейросеть с нуля}

% Выбрать одно из трех:
% КТ1 -- \setStageOne
% КТ2 -- \setStageTwo
% Финальная версия -- \setStageFinal
%\setStageOne
%\setStageTwo
\setStageFinal

\setGroup{211}
% Сюда можно воткнуть картинку подписи с помощью \includegraphics[scale=0.2]{2023-02-07 22.29.16.jpg}
% (scale подбирается индивидуально для конкретной картинки)
%\setStudentSgn{\includegraphics[scale=0.02]{sign.jpg}}
\setStudent{Д. А. Сорокин}
\setStudentDate{05.02.2023}
\setAdvisor{Дмитрий Витальевич Трушин}
\setAdvisorTitle{доцент, к.ф.-м.н.}
\setAdvisorAffiliation{ФКН НИУ ВШЭ}
\setAdvisorDate{12.06}
\setGrade{11}
% Сюда можно воткнуть картинку подписи с помощью \includegraphics[scale=0.2]{<имя файла>}
% (scale подбирается индивидуально для конкретной картинки)
%\setAdvisorSgn{\includegraphics[scale=0.5]{tru.png}}
\setYear{2023}


% С этого момента начинается текст документа
\begin{document}

% Эта команда создает титульную страницу
\makeTitlePage

% Здесь будет автоматически генерироваться содержание документа
\tableofcontents

% Данное окружение оформляет аннотацию: краткое описание текста выделенным абзацем после заголовка
% \begin{abstract}
% Текст аннотации. Здесь кратко в два-три предложения описываем, что происходит в работе.
% \end{abstract}


\section{Введение}
Основная задача данного проекта это изучить базовые принцыпы работы
нейронных сетей и создать собственную реализацию библиотеки для работы с нейронными 
сетями на языке C++. В данном случае под нейронной сетью подразумевается метод 
машинного обучения, который широко используется для построения моделей и прогнозирования.
Нейронная сеть представляет слои, состоящие из вычислительных узлов, связанных между 
собой. 

Спектр применения нейронных сетей крайне велик, но мы рассмотрим общую формулировку:\\
Есть векторные пространства $\R^n, \R^m$ и неизвестная функция $\phi: \R^n \to \R^m$, 
так же есть множество примеров - пары $(x_i, y_i = \phi(x_i))$ наша цель научиться 
предсказывать $\phi(x_i)$ наиболее точно. 


Результатом работы будет являться библиотека на языке C++, 
для построения и обучения своих нейронных сетей. 

Прежде чем переходить к описанию программной части, 
стоит дать более точное описания нашего алгоритма.

\section{Описание структуры нейронной сети}
Для начала введем функцию потерь - это функция будет 
показывать насколько наше предсказание близко к искомой функции,
например можем использовать среднее арифметическое отклонений на каждой 
паре тестовых данных:
\begin{align*}
    \Sigma_{i=1}^{n}\frac{L(\psi(x_i), y_i)}{n}
\end{align*}
Наша цель - минимизировать функцию потерь.\\
Нейронная сеть будет состоять из линейных слоев, устроенных следующим образом:\\
Это функция $\phi'(x) : \R^{n_1} \to \R^{n_2}, \phi'(x) = Ax + b$, где A и b
являются параметрами. Также после каждого линейного слоя идет нелинейный:\\
\begin{align*}
    \sigma(x) \R^{n} \to \R^{n}\\
    \sigma\begin{pmatrix}
        x_1\\
        x_2\\
        ...\\
        x_n
    \end{pmatrix}
    =\begin{pmatrix}
        \sigma(x_1)\\
        \sigma(x_2)\\
        ...\\
        \sigma(x_n)
    \end{pmatrix}
\end{align*}
Где $\sigma(x)$ некоторая нелинейная функция, подробнее про конкретные примеры 
$\sigma$ в програмной части.\\
Нелинейные слои необходимы, иначе комбинация слоев будет всегда являться линейной функцией.\\
Комбинация слоев и будет являться нейронной сетью. Осталось определить процесс обучения.\\
\section{Описание алгоритма обучения}
Воспользуемся градиентным спуском - идея в том, что нашу нейронную сеть 
можно представить как сложную функцию зависящую от многих параметров($A_i, b_i$ в линейных слоях)
тогда если мы посчитаем производную функции потерь от этих переменных и сдвинемся в сторону противоположную 
градиенту, мы уменьшим значение функции потерь. Повторив так некоторое количество раз, мы окажемся в
локальном минимуме.

Теперь посчитаем градиент, основываясь только на одной паре 
$x_i, y_i$:

$\phi(F(x_i), y_i)$ - наша функция потерь, $F$ - композиция всех слоев сети,
$\theta$ - все параметры сети, $\phi(x_i) = \omega_i$
\begin{align*}
    \frac{\delta}{\delta \theta}\phi(F(x_i), y_i) =
    \frac{\delta \phi}{\delta \theta} (w_i, y_i) \frac{\delta F}{\delta \theta} (x_i)\\
    \frac{\delta \phi}{\delta \theta} (w_i, y_i) = u_i 
\end{align*}
Рассмотрим последний слой нейронной сети, состоящий из композиции линейного и нелинейного:
\begin{align*}
    \R^m \xrightarrow[]{g} \R^n \xrightarrow[]{\sigma} \R^n\\
    z_i \to A z_i + b \to \sigma(A z_i + b)
\end{align*}
Где $z_i$ - результат преобразования $x_i$ предыдущими слоями.\\
\begin{align*}
    d(\sigma(Az + b)) = d\sigma(dA z + Adz + db)
\end{align*}
Вернемся к $\frac{\delta}{\delta \theta}\phi(F(x_i), y_i)$:\\
\begin{align*}
    \frac{\delta}{\delta \theta}\phi(F(x_i), y_i) = 
    u_i * d\sigma(dA z + Adz + db) 
\end{align*}
Перепишем это в таком формате:\\
\begin{align*}
    u_i d(\sigma) d(A) z + u_i d(\sigma)A d(z) + 
    u_i d(\sigma) db
\end{align*}
Т.к это одномерное выражение справедливо:\\
$u_i d(\sigma) d(A) z = tr(u_i d(\sigma) d(A) z) = 
tr(z u_i d(\sigma) d(A))$\\
Тогда по правилам \href{https://academy.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie}{матричного дифференцировния}
$(z u_i d(\sigma))^T = \frac{d(F\circ \phi)}{dA}$\\
$(u_i d(\sigma))^T = \frac{d(F\circ \phi)}{db}$\\
И наконец $u_i' =u_i d(\sigma)A$ - значение $u_i$ которое будет использовано 
для вычисления градиента в предыдущих слоях.

$d(\sigma)$ в свою очередь является диагональной матрицей в силу устройства 
$\sigma$ и значения на диагонали вычисляются по формуле
$\sigma'(A z_i + b)_i$.

Так мы научились последовательно вычислять градиент 
по слоям для одной пары $(x_i, y_i)$. 

Заметем, что вычисления градиента по батчу пар не сложнее - достаточно вычислить градиент для 
k пар, затем сложить градиенты и поделить на k.
\newpage
\section{Описание функциональных и нефункциональных требований к программному проекту}

Результатом проекта является библиотек для создания/обучения нейронных сетей, написанная 
на языке C++.\\
Нейронная сеть состоит из следующих частей:\\
\begin{itemize}
    \item  \textbf{Линейный слой} - 
    класс представления линейного слоя нейронной сети, обладающий следующими методами:
\begin{itemize}
    \item \textbf{CalculateByX(x)} - применяет слой к вектору x и возвращает результат вычисления
    \item \textbf{CalculateDerivative(u)} - вычисляет dA, db и u' по посчитанному ранее u.
\end{itemize}
    \item  \textbf{Нелинейный слой} - 
    класс предстатвления нелинейного слоя, на основе одной из нелинейных функций. Методы:
\begin{itemize}
    \item \textbf{CalculateByX(x)} - применяет слой к вектору x и возвращает результат вычисления
    \item \textbf{CalculateDerivative(x)} - вычисляет производную
\end{itemize}
    \item  \textbf{Класс представления нейронной сети} - создает нейронную сеть, с заданным количеством слоев 
    и заданными нелинейными функциями. Методы:
    \begin{itemize}
        \item \textbf{Train} - реализация градиентного спуска
    \end{itemize} 
    \item  \textbf{Вспомогательные классы}
\begin{itemize} 
    \item \textbf{Классы поддерживаемых нелинейных функций} 
    \item \textbf{Классы для функций потерь}
\end{itemize}
\end{itemize}

\section{Инструменты, используемые в проекте}
При выполнении проекта мы необходима  \href{https://en.cppreference.com/w/}{документация C++}.\\
Так же будет использоваться библиотека для матричных вычислений \href{https://eigen.tuxfamily.org/dox/GettingStarted.html}{Eigen}.\\

% \section{Содержательная часть}

% Здесь идет планомерное изложение информации от начала до конца. Тут не нужна никакая философия или объяснения, все это было во введении. Тут сухой математический текст с определениями, формулировками и где надо доказательствами. Содержательную часть можно бить на части, чтобы структурировать изложение.

% \subsection{Содержательная часть 1}

% \subsection{Содержательная часть 2}

% % Здесь автоматически генерируется библиография. Первая команда задает стиль оформления библиографии, а вторая указывает на имя файла с расширением bib, в котором находится информация об источниках.
\bibliographystyle{plainurl}
\bibliography{bibl}



% % С этого момента глобальная нумерация идет буквами. Этот раздел я добавил лишь для демонстрации возможностей LaTeX, его можно и нужно удалить и он не нужен для курсового проекта непосредственно.
% \appendix

% Проведем небольшой обзор возможностей \LaTeX. Далее идет обзорный кусок, который надо будет вырезать. Он приведен лишь для демонстрации возможностей \LaTeX.

% \section{Нумеруемый заголовок}
% Текст раздела
% \subsection{Нумеруемый подзаголовок}
% Текст подраздела
% \subsubsection{Нумеруемый подподзаголовок}
% Текст подподраздела

% \section*{Не нумеруемый заголовок}
% Текст раздела
% \subsection*{Не нумеруемый подзаголовок}
% Текст подраздела
% \subsubsection*{Не нумеруемый подподзаголовок}
% Текст подподраздела


% \paragraph{Заголовок абзаца} Текст абзаца

% Формулы в тексте набирают так $x = e^{\pi i}\sqrt{\text{формула}}$. Выключенные не нумерованные формулы набираются либо так:
% \[
% x = e^{\pi i}\sqrt{\text{формула}}
% \]
% Либо так
% $$
% x = e^{\pi i}\sqrt{\text{формула}}
% $$
% Первый способ предпочтительнее при подаче статей в журналы AMS, потому рекомендую привыкать к нему.

% Выключенные нумерованные формулы:
% \begin{equation}\label{Equation1}
% % \label{имя-метки} эта команда ставит метку, на которую потом можно сослаться с помощью \ref{имя-метки}. Метки можно ставить на все объекты, у которых есть автоматические счетчики (номера разделов, подразделов, теорем, лемм, формул и т.д.
% x = e^{\pi i}\sqrt{\text{формула}}
% \end{equation}
% Или не нумерованная версия
% \begin{equation*}
% x = e^{\pi i}\sqrt{\text{формула}}
% \end{equation*}

% Уравнение~\ref{Equation1} радостно занумеровано.

% Лесенка для длинных формул
% \begin{multline}
% x = e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% \tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% \cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{multline}

% Многострочная формула с центровкой
% \begin{gather}
% x = e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% \tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% \cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{gather}

% Многострочная формула с ручным выравниванием. Выравнивание идет по знаку $\&$, который на печать не выводится.
% \begin{align}
% x = &e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% &\tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% &\cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{align}

% \begin{theorem}
% Текст теоремы
% \end{theorem}
% \begin{proof}
% В специальном окружении оформляется доказательство.
% \end{proof}

% \begin{theorem}[Имя теоремы]
% Текст теоремы
% \end{theorem}
% \begin{proof}[Доказательство нашей теоремы]
% В специальном окружении оформляется доказательство.
% \end{proof}

% \begin{definition}
% Текст определения
% \end{definition}

% \begin{remark}
% Текст замечания
% \end{remark}

% \paragraph{Перечни:} Нумерованные
% \begin{enumerate}
% \item Первый
% \item Второй
% \begin{enumerate}
% \item Вложенный первый
% \item Вложенный второй
% \end{enumerate}
% \end{enumerate}

% Не нумерованные

% \begin{itemize}
% \item Первый
% \item Второй
% \begin{itemize}
% \item Вложенный первый
% \item Вложенный второй
% \end{itemize}
% \end{itemize}


% Здесь текст документа заканчивается
\end{document}
% Начиная с этого момента весь текст LaTeX игнорирует, можете вставлять любую абракадабру.
